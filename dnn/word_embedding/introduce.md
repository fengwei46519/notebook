
**word embedding**
>Vector Representations of Words.给出一个文档，文档就是一个单词序列比如 “A B A C B F G”, 希望对文档中每个不同的**单词**都得到一个对应的**向量**(往往是低维向量)表示。

tensorflow(使用 word2vec， https://www.tensorflow.org/tutorials/word2vec)

常见的word embedding的方法有:
1. Distributed Representations of Words and Phrases and their Compositionality
2. Efficient Estimation of Word Representations in Vector Space
3. GloVe Global Vectors forWord Representation
4. Neural probabilistic language models
5. Natural language processing (almost) from scratch
6. Learning word embeddings efficiently with noise contrastive estimation
7. A scalable hierarchical distributed language model
8. Three new graphical models for statistical language modelling
9. Improving word representations via global context and multiple word prototypes

